{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2402e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f7c02",
   "metadata": {},
   "source": [
    "### Scraping Sites Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef5d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Markets site\n",
    "def scrapeMarketsSite(weblink):\n",
    "    \n",
    "    req2 = requests.get(weblink)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='at-headline').find('h1', class_='typography__StyledTypography-owin6q-0 kVbxLR').text.strip()\n",
    "    \n",
    "    date = soup2.find('div', class_='at-created label-with-icon').find('span', class_='typography__StyledTypography-owin6q-0 fUOSEs').text.strip()\n",
    "    date = datetime.strptime(date[:12], '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "    \n",
    "    content =soup2.find('div', class_='contentstyle__StyledWrapper-g5cdrh-0 jsJHBz').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290bb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Podcasts site\n",
    "def scrapePodcastSite(weblink):\n",
    "\n",
    "    req2 = requests.get(weblink)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='episode-title').find('h1', class_='typography__StyledTypography-owin6q-0 hjHKEC').text.strip()\n",
    "\n",
    "    date = soup2.find('div', class_='episode-published-date').find('span', class_='typography__StyledTypography-owin6q-0 bZvpZH').text.strip()\n",
    "    date = datetime.strptime(date, '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "    content =soup2.find('div', class_='contentstyle__StyledWrapper-g5cdrh-0 jsJHBz').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c54f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Video site\n",
    "def scrapeVideoSite(weblink):\n",
    "\n",
    "    req2 = requests.get(weblink)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='Box-sc-1hpkeeg-0 dNNZxF').find('h1', class_='typography__StyledTypography-owin6q-0 kzGyYF video-title').text.strip()\n",
    "\n",
    "    date = soup2.find('span', class_='typography__StyledTypography-owin6q-0 fUOSEs video-date').text.strip()\n",
    "    date = datetime.strptime(date, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "    content =soup2.find('div', class_='show-morestyles__ContentWrapper-jwdjew-0 epVFdw').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d7e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape TV site\n",
    "def scrapeTVSite(weblink):\n",
    "    \n",
    "    req2 = requests.get(weblink)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='Box-sc-1hpkeeg-0 dNNZxF').find('h1', class_='typography__StyledTypography-owin6q-0 kzGyYF video-title').text.strip()\n",
    "\n",
    "    date = soup2.find('span', class_='typography__StyledTypography-owin6q-0 fUOSEs video-date').text.strip()\n",
    "    date = datetime.strptime(date, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "    content =soup2.find('div', class_='show-morestyles__ContentWrapper-jwdjew-0 cLoGZO').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3015aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Consensus site\n",
    "def scrapeConsensusSite(weblink): \n",
    "    \n",
    "    req2 = requests.get(weblink)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='at-headline').find('h1', class_='typography__StyledTypography-owin6q-0 kVbxLR').text.strip()\n",
    "\n",
    "    date = soup2.find('span', class_='typography__StyledTypography-owin6q-0 fUOSEs').text.strip()\n",
    "    date = datetime.strptime(date[:12], '%b %d, %Y').strftime('%Y-%m-%d')  \n",
    "\n",
    "    content =soup2.find('div', class_='contentstyle__StyledWrapper-g5cdrh-0 jsJHBz').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d58bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Story site\n",
    "def scrapeStorySite(weblink): \n",
    "    \n",
    "    req2 = requests.get(weblink)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='at-headline').find('h1', class_='typography__StyledTypography-owin6q-0 kVbxLR').text.strip()\n",
    "\n",
    "    date = soup2.find('span', class_='typography__StyledTypography-owin6q-0 fUOSEs').text.strip()\n",
    "    date = datetime.strptime(date[:12], '%b %d, %Y').strftime('%Y-%m-%d')  \n",
    "\n",
    "    content =soup2.find('div', class_='at-content-wrapper').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3426a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Learn site\n",
    "def scrapeLearnSite(weblink): \n",
    "    \n",
    "    req2 = requests.get(weblink)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='at-headline headline2').find('h1', class_='typography__StyledTypography-owin6q-0 fPbJUO').text.strip()\n",
    "\n",
    "    date = soup2.findAll('meta', property='article:published_time')[0][\"content\"][:10]\n",
    "\n",
    "    content =soup2.find('div', class_='at-content-wrapper').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967e2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Vidoe site 2\n",
    "def scrapeVideo2Site(weblink):\n",
    "    \n",
    "    req2 = requests.get(weblinkk)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='at-headline').find('h1', class_='typography__StyledTypography-owin6q-0 kVbxLR').text.strip()\n",
    "\n",
    "    date = soup2.findAll('meta', property='article:published_time')[0][\"content\"][:10]\n",
    "\n",
    "    content =soup2.find('div', class_='at-content-wrapper').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42ca57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape TV site 2\n",
    "def scrapeTV2Site(weblink):\n",
    "    \n",
    "    req2 = requests.get(weblink)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='Box-sc-1hpkeeg-0 dNNZxF').find('h1', class_='typography__StyledTypography-owin6q-0 kzGyYF video-title').text.strip()\n",
    "\n",
    "    date = soup2.find('span', class_='typography__StyledTypography-owin6q-0 fUOSEs video-date').text.strip()\n",
    "    date = datetime.strptime(date, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "    content =soup2.find('div', class_='show-morestyles__ContentWrapper-jwdjew-0 fQpATL').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "020183ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Other site\n",
    "def scrapeOtherSite(weblink): \n",
    "    \n",
    "    req2 = requests.get(weblink)\n",
    "    soup2 = bs(req2.text, 'html.parser')\n",
    "\n",
    "    title = soup2.find('div', class_='at-headline').find('h1', class_='typography__StyledTypography-owin6q-0 kVbxLR').text.strip()\n",
    "\n",
    "    date = soup2.find('span', class_='typography__StyledTypography-owin6q-0 fUOSEs').text.strip()\n",
    "    try:\n",
    "        date = datetime.strptime(date[:11], '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        try: \n",
    "            date = datetime.strptime(date[:11], '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            date = date\n",
    "\n",
    "    content =soup2.find('div', class_='at-content-wrapper').findAll('p')\n",
    "    content = \" \".join(list(map(lambda x: x.text.strip(), content)))\n",
    "    \n",
    "    return title, date, content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a31d42",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ccdaa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Date, Content, Page]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Create empty dataframe\n",
    "df = pd.DataFrame({'Title':[], 'Date':[], 'Content':[], 'Page':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddaac4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 752/752 [30:32<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through bitcoin searches and scrape all relevant links\n",
    "browser=webdriver.Chrome()\n",
    "\n",
    "for pageNum in tqdm(range(1355)):\n",
    "    url = f\"https://www.coindesk.com/search?s=bitcoin&sort=1&i={pageNum}\"\n",
    "    browser.get(url) #navigate to the page\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    \n",
    "    soup = bs(innerHTML, 'html.parser')\n",
    "    articles = soup.findAll(\"div\", class_=\"searchstyles__SearchCardWrapper-ci5zlg-21 hZygnS\")\n",
    "    for article in articles:\n",
    "        links = article.findAll(\"a\", class_=\"Box-sc-1hpkeeg-0 hBnhmi\")\n",
    "        if len(links)==2:\n",
    "            link = links[1][\"href\"]\n",
    "            try:\n",
    "                if link[1:6]==\"video\":\n",
    "                    try:\n",
    "                        title, date, content = scrapeVideoSite(\"https://www.coindesk.com\" + link)\n",
    "                    except:\n",
    "                        title, date, content = scrapeVideo2Site(\"https://www.coindesk.com\" + link)\n",
    "                elif link[1:3]==\"tv\":\n",
    "                    try:\n",
    "                        title, date, content = scrapeTVSite(\"https://www.coindesk.com\" + link)\n",
    "                    except:\n",
    "                        title, date, content = scrapeTV2Site(\"https://www.coindesk.com\" + link)\n",
    "                elif link[1:9]==\"podcasts\":\n",
    "                    title, date, content = scrapePodcastSite(\"https://www.coindesk.com\" + link)\n",
    "                elif link[1:19]==\"consensus-magazine\":\n",
    "                    title, date, content = scrapeConsensusSite(\"https://www.coindesk.com\" + link)\n",
    "                elif link[1:6]==\"learn\":\n",
    "                    title, date, content = scrapeLearnSite(\"https://www.coindesk.com\" + link)\n",
    "                else:\n",
    "                    title, date, content = scrapeArticle(\"https://www.coindesk.com\" + link)\n",
    "                df = pd.concat([df, pd.Series({'Title':title, 'Date':date, 'Content':content, 'Page':pageNum}).to_frame().T], ignore_index=True)\n",
    "            except:\n",
    "                try:\n",
    "                    try:\n",
    "                        title, date, content = scrapeStorySite(\"https://www.coindesk.com\" + link)\n",
    "                    except:\n",
    "                        title, date, content = scrapeOtherSite(\"https://www.coindesk.com\" + link)\n",
    "                    df = pd.concat([df, pd.Series({'Title':title, 'Date':date, 'Content':content, 'Page':pageNum}).to_frame().T], ignore_index=True)\n",
    "                except:\n",
    "                    df = pd.concat([df, pd.Series({'Title':link, 'Date':None, 'Content':None, 'Page':pageNum}).to_frame().T], ignore_index=True)\n",
    "    \n",
    "    # Save every 100 pages\n",
    "    # if pageNum % 100 == 0:\n",
    "    #     df.to_csv(\"coin desk news \" + str(pageNum-100) + \" to \" + str(pageNum) + \".csv\")\n",
    "    #     df = df.iloc[0:0]\n",
    "\n",
    "df.to_csv(\"CoinDesk News.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588bf6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
